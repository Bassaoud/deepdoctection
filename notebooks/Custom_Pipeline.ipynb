{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a custom pipeline\n",
    "\n",
    "The **deep**doctection analyzer is an example of a Document Layout Analysis pipeline. In this tutorial we'll show you the concepts so that you can build a pipeline youself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import deepdoctection as dd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is not that difficult: There are models that fulfill a given task, there are pipeline components or pipeline backbones that invoke models and take care of pre- and post-processing results. There are also pipeline backbones that do not invoke models but only consolidate results. \n",
    "\n",
    "And there is the pipeline that puts everything together.\n",
    "\n",
    "## Catalog and Registries\n",
    "\n",
    "You can get the essential information for pre-trained model from the `ModelCatalog`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janis/Public/deepdoctection_pt/venv/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "\u001b[32m[1110 18:54.44 @file_utils.py:33]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[37mPyTorch version 1.9.0+cu111 available.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m╒════════════════════════════════════════════════════╕\n",
      "│ name                                               │\n",
      "╞════════════════════════════════════════════════════╡\n",
      "│ layout/model-800000_inf_only.data-00000-of-00001   │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ cell/model-1800000_inf_only.data-00000-of-00001    │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ item/model-1620000_inf_only.data-00000-of-00001    │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ item/model-1620000.data-00000-of-00001             │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ layout/model-800000.data-00000-of-00001            │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ cell/model-1800000.data-00000-of-00001             │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ layout/d2_model-800000-layout.pkl                  │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ layout/d2_model_0829999_layout_inf_only.pt         │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ layout/d2_model_0829999_layout.pth                 │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ cell/d2_model-1800000-cell.pkl                     │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ cell/d2_model_1849999_cell_inf_only.pt             │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ cell/d2_model_1849999_cell.pth                     │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ item/d2_model-1620000-item.pkl                     │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ item/d2_model_1639999_item.pth                     │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ item/d2_model_1639999_item_inf_only.pt             │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ microsoft/layoutlm-base-uncased/pytorch_model.bin  │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ microsoft/layoutlm-large-uncased/pytorch_model.bin │\n",
      "├────────────────────────────────────────────────────┤\n",
      "│ fasttext/lid.176.bin                               │\n",
      "╘════════════════════════════════════════════════════╛\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "dd.print_model_infos(add_description=False,add_config=False,add_categories=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's select fasttext language detector. We need the categories the model predicts and the model wrapper. `fasttext/lid.176.bin` is just an artefact. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories=dd.ModelCatalog.get_profile(\"fasttext/lid.176.bin\").categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FasttextLangDetector'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.ModelCatalog.get_profile(\"fasttext/lid.176.bin\").model_wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download `lid.176.bin` with help of the `ModelDownloadManager`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1110 19:02.34 @fs.py:94]\u001b[0m  \u001b[32mINF\u001b[0m  \u001b[37mFile lid.176.bin exists! Skip download.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "path_weights=dd.ModelDownloadManager.maybe_download_weights_and_configs(\"fasttext/lid.176.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model wrapper\n",
    "\n",
    "We know from the `ModelCatalog` which wrapper we must use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "fast_text = dd.FasttextLangDetector(path_weights, categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not done yet, because we still need to choose how to extract text. Let's simply stick to Tesseract and use the default english setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tess_ocr_config_path = dd.get_configs_dir_path() / \"dd/conf_tesseract.yaml\"  # This file will be in you .cache if you ran the analyzer before. \n",
    "# Otherwise make sure to copy the file from 'configs/conf_tesseract.yaml'\n",
    "\n",
    "tesseract_ocr = dd.TesseractOcrDetector(tess_ocr_config_path.as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline backbone\n",
    "\n",
    "As with models et all. we have a pipeline component registry. Having this starting point we can select the right backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'SubImageLayoutService': deepdoctection.pipe.cell.SubImageLayoutService,\n",
       " 'ImageCroppingService': deepdoctection.pipe.common.ImageCroppingService,\n",
       " 'MatchingService': deepdoctection.pipe.common.MatchingService,\n",
       " 'PageParsingService': deepdoctection.pipe.common.PageParsingService,\n",
       " 'LanguageDetectionService': deepdoctection.pipe.language.LanguageDetectionService,\n",
       " 'ImageLayoutService': deepdoctection.pipe.layout.ImageLayoutService,\n",
       " 'LMTokenClassifierService': deepdoctection.pipe.lm.LMTokenClassifierService,\n",
       " 'LMSequenceClassifierService': deepdoctection.pipe.lm.LMSequenceClassifierService,\n",
       " 'TableSegmentationRefinementService': deepdoctection.pipe.refine.TableSegmentationRefinementService,\n",
       " 'TableSegmentationService': deepdoctection.pipe.segment.TableSegmentationService,\n",
       " 'TextExtractionService': deepdoctection.pipe.text.TextExtractionService,\n",
       " 'TextOrderService': deepdoctection.pipe.text.TextOrderService,\n",
       " 'SimpleTransformService': deepdoctection.pipe.transform.SimpleTransformService}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dd.pipeline_component_registry.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_detect_comp = dd.LanguageDetectionService(fast_text,text_detector=tesseract_ocr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now build our very simple pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = dd.DoctectionPipe(pipeline_component_list=[lang_detect_comp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline\n",
    "\n",
    "We use the DoctectionPipe, which already contains functions for loading and outputting the extracts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = Path(dd.get_package_path()) / \"notebooks/pics/samples/sample_3\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](./pics/samples/sample_3/sample_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build the pipeline by calling the analyze method and want the results returned as an image. An image is the core object where everything grapped from detectors and pipeline components is stored. \n",
    "\n",
    "Note, that the default output \"page\" will not return anything, as this type requires additional layout detections which we will adress later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline.analyze(path=path, output=\"image\")\n",
    "df.reset_state()\n",
    "doc = next(iter(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not make much sense to dig deeper into the image structure. It is important to know, that it captures all fine graded information from the OCR result in an ImageAnnotation object. E.g. each single word is stored with some uuid, bounding box and value (the recorded text). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(545,\n",
       " ImageAnnotation(active=True, _annotation_id='3be39a8e-880b-3a18-b0d7-80e05beb68f4', category_name=<LayoutType.word>, _category_name=<LayoutType.word>, category_id='1', score=0.9221703338623047, sub_categories={<WordType.characters>: ContainerAnnotation(active=True, _annotation_id='e68e2072-ff7c-3152-ab6b-d8fc6156dc02', category_name=<WordType.characters>, _category_name=<WordType.characters>, category_id='None', score=0.9221703338623047, sub_categories={}, relationships={}, value='Anleihemärkte')}, relationships={}, bounding_box=BoundingBox(absolute_coords=True, ulx=134.921634465456, uly=157.1062769368291, lrx=472.318872153759, lry=195.05085966736078, height=37.94458273053169, width=337.397237688303)))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc.annotations), doc.annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding layout elements\n",
    "\n",
    "The current information does not help much so far. An arrangement of word coordinates from left to right would not result in a meaningful reading order, as the layout incorporates several columns. One rather has to determine additional text blocks that frame individual columns. A built-in layout detector and the associated ImageLayoutService as a pipeline component are suitable for this.\n",
    "\n",
    "At this point it starts to depend on whether the DL framework Tensorflow or PyTorch will be used. We assume that Tensorflow is installed, hence we need to import the Tensorflow related Detector TPFrcnnDetector. Use D2FrcnnDetector for PyTorch.\n",
    "\n",
    "We use the model config and the weights of the built-in analyzer. If you haven't got through the starter tutorial you can download weights using the ModelDownloadManager. \n",
    "\n",
    "```\n",
    "from deepdoctection.extern.model import ModelDownloadManager\n",
    "ModelDownloadManager.maybe_download_weights_and_configs(\"layout/model-800000_inf_only.data-00000-of-00001\")\n",
    "```\n",
    "\n",
    "Download `\"layout/d2_model-800000-layout.pkl\"` instead, in case you use PyTorch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdoctection.extern import TPFrcnnDetector, ModelCatalog    \n",
    "from deepdoctection.pipe import ImageLayoutService\n",
    "from deepdoctection.utils.systools import get_weights_dir_path, get_configs_dir_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the model is downloaded from the hub, both the weights and the config file are loaded into the cache. The paths to both files are required\n",
    "in order to instantiate the detector. You can use the ModelCatalog to build the path. Moreover, the ModelCatalog\n",
    "provides a brief model card of all registered models.\n",
    "\n",
    "It is also necessary to pass a dict with the category-id/category names pairs. This mapping is standard and results from the dataset Publaynet on which this model was trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name': 'layout/model-800000_inf_only.data-00000-of-00001',\n",
       " 'description': 'Tensorpack layout model for inference purposes trained on Publaynet',\n",
       " 'size': [274552244, 7907],\n",
       " 'tp_model': True,\n",
       " 'config': 'dd/tp/conf_frcnn_layout.yaml',\n",
       " 'hf_repo_id': 'deepdoctection/tp_casc_rcnn_X_32xd4_50_FPN_GN_2FC_publaynet_inference_only',\n",
       " 'hf_model_name': 'model-800000_inf_only',\n",
       " 'hf_config_file': ['conf_frcnn_layout.yaml'],\n",
       " 'urls': None,\n",
       " 'categories': {'1': <LayoutType.text>,\n",
       "  '2': <LayoutType.title>,\n",
       "  '3': <LayoutType.list>,\n",
       "  '4': <LayoutType.table>,\n",
       "  '5': <LayoutType.figure>}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profile = ModelCatalog.get_profile(\"layout/model-800000_inf_only.data-00000-of-00001\")\n",
    "profile.as_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_yaml_path = ModelCatalog.get_full_path_configs(\"layout/model-800000_inf_only.data-00000-of-00001\")\n",
    "weights_path = ModelCatalog.get_full_path_weights(\"layout/model-800000_inf_only.data-00000-of-00001\") \n",
    "categories_layout = profile.categories\n",
    "layout_detector = TPFrcnnDetector(\"layout\", config_yaml_path,weights_path,categories_layout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ImageLayoutService does need a detector and an additional attribute that we will not discuss here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "layout_service = ImageLayoutService(layout_detector,to_image=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting text and layouts are independent tasks, hence the can be placed in any order within the component. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_component_list.append(layout_service)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's rebuild a new pipeline and start the process again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DoctectionPipe(pipeline_component_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline.analyze(path=path, output=\"image\")\n",
    "df.reset_state()\n",
    "doc = next(iter(df))\n",
    "len(doc.annotations), doc.annotations[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add matching and reading order\n",
    "\n",
    "Now, that layout and words can be extracted we now have to assign each detected word to a text box (if this is possible). For that we use the pre built MatchingService. In our configuration child categories have to be mapped to parent categories. We use a intersection over are matching rule with a threshold of 0.9. In other terms, if a word box overlays with at least 0.9 of its area to a text block it will be assigned to that box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdoctection.pipe import MatchingService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_service = MatchingService(parent_categories=[\"text\",\"title\",\"cell\",\"list\",\"table\",\"figure\"],\n",
    "                        child_categories=\"word\",\n",
    "                        matching_rule=\"ioa\",\n",
    "                        threshold=0.9)\n",
    "\n",
    "pipeline_component_list.append(matching_service )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading order service has a straight forward setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepdoctection.pipe import TextOrderService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "reading_order_service = TextOrderService(text_container=\"word\",floating_text_block_names=[\"text\",\"title\",\"list\"],\n",
    "                                         text_block_names=[\"text\",\"title\",\"list\",\"table\",\"figure\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_component_list.append(reading_order_service)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = DoctectionPipe(pipeline_component_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can eventually fire up the custom build analyzer. As we have everything we need to build the lightweight page object we can change the output accordingly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pipeline.analyze(path=path, output=\"page\")\n",
    "df.reset_state()\n",
    "page = next(iter(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can eventually print the OCRed text in reading order with the get_text method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Anleihemärkte im Geschäftsjahr bis zum 31.12.2018\n",
      "Die internationalen Anleihe- märkte entwickelten sich im Geschäftsjahr 2018 unter- schiedlich und phasenweise sehr volatil. Dabei machte sich bei den Investoren zunehmend Nervosität breit, was in steigen- den Risikoprämien zum Aus- druck kam. Grund hierfür waren Turbulenzen auf der weltpoli- tischen Bühne, die die politi- schen Risiken erhöhten. Dazu zählten unter anderem populis- tische Strömungen nicht nur in den USA und Europa, auch in den Emerging Markets, wie zuletzt in Brasilien und Mexiko, wo Populisten in die Regie- rungen gewählt wurden. Der eskalierende Handelskonflikt zwischen den USA einerseits sowie Europa und China ande- rerseits tat sein übriges. Zudem ging Italien im Rahmen seiner Haushaltspolitik auf Konfronta- tionskurs zur Europäischen Uni- on (EU). Darüber hinaus verun- sicherte weiterhin der drohende Brexit die Marktteilnehmer, insbesondere dahingehend, ob der mögliche Austritt des Ver- einigten Königreiches aus der EU geordnet oder ohne ein Übereinkommen ungeordnet vollzogen wird. Im Gegensatz den politischen Unsicher- heiten standen die bislang eher zuversichtlichen, konventionel- len Wirtschaftsindikatoren So expandierte die Weltwirtschaft kräftig, wenngleich sich deren Wachstum im Laufe der zwei- ten Jahreshälfte 2018 etwas verlangsamte. Die Geldpolitik war historisch gesehen immer noch sehr locker, trotz der welt- weit sehr hohen Verschuldung und der Zinserhöhungen der US-Notenbank.\n",
      "Entwicklung der Leitzinsen in den USA und im Euroraum % p. a.\n",
      "Zinswende nach Rekordtiefs bei Anleiherenditen? Im Berichtszeitraum kam es an den Anleihemärkten - wenn auch uneinheitlich und unter- schiedlich stark ausgeprägt unter Schwankungen zu stei- genden Renditen auf teilweise immer noch sehr niedrigem Niveau, begleitet von nachge- benden Kursen. Dabei konnten sich die Zinsen vor allem in den USA weiter von ihren histori- schen Tiefs lösen. Gleichzeitig wurde die Zentralbankdivergenz zwischen den USA und dem Euroraum immer deutlicher. An- gesichts des Wirtschaftsbooms in den USA hob die US-Noten- bank Fed im Berichtszeitraum den Leitzins in vier Schritten weiter um einen Prozentpunkt auf einen Korridor von 2,25% 2,50% p.a. an. Die Europäische Zentralbank (EZB) hingegen hielt an ihrer Nullzinspolitik fest und die Bank of Japan beließ ihren Leitzins bei -0,10% p.a. Die Fed begründete ihre Zinser- höhungen mit der Wachstums- beschleunigung und der Voll- beschäftigung am Arbeitsmarkt in den USA. Zinserhöhungen ermöglichten der US-Notenbank einer Überhitzung der US-Wirt- schaft vorzubeugen, die durch die prozyklische expansive\n",
      "Fiskalpolitik des US-Präsidenten Donald Trump in Form von Steuererleichterungen und einer Erhöhung der Staatsausgaben noch befeuert wurde. Vor die- sem Hintergrund verzeichneten die US-Bondmärkte einen spür- baren Renditeanstieg, der mit merklichen Kursermäßigungen einherging. Per saldo stiegen die Renditen zehnjähriger US- Staatsanleihen auf Jahressicht von 2,4% p.a. auf 3,1% p.a.\n",
      "Diese Entwicklung in den USA hatte auf den Euroraum jedoch nur phasenweise und partiell, insgesamt aber kaum einen zinstreibenden Effekt auf Staats- anleihen aus den europäischen Kernmärkten wie beispielsweise Deutschland und Frankreich. So gaben zehnjährige deutsche Bundesanleihen im Jahresver- lauf 2018 unter Schwankungen per saldo sogar von 0,42% p.a. auf 0,25% p. a. nach. Vielmehr standen die Anleihemärkte der Euroländer insbeson- dere ab dem zweiten Quartal 2018 unter dem Einfluss der politischen und wirtschaftlichen Entwicklung in der Eurozone, vor allem in den Ländern mit hoher Verschuldung und nied- rigem Wirtschaftswachstum In den Monaten Mai und Juni\n"
     ]
    }
   ],
   "source": [
    "print(page.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to continue\n",
    "\n",
    "In the next step we recommend the tutorial **Datasets_and_Eval**. Here, the data model of the package is explained in more detail. It also explains how to evaluate the precision of models using labeled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-doc-pt",
   "language": "python",
   "name": "deep-doc-pt"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
