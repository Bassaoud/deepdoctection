{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We show how a model can be fine-tuned for a specific task and how the performance can be compared with the pre-trained model.\n",
    "\n",
    "For this purpose, we want to try to improve the table extraction in the Deep-Doctection Analyzer as an example. To better understand what we are trying to address, we need to say a little more about processing table extraction.\n",
    "\n",
    "\n",
    "![title](./pics/dd_table.png)\n",
    "\n",
    "\n",
    "Table extraction is carried out in different stages:\n",
    "\n",
    "- Table detection\n",
    "- Cell detection\n",
    "- Row and column detection\n",
    "- Segmentation / cell labeling\n",
    "\n",
    "Tables, cells and rows / columns are recognized with object detectors (Cascade-RCNN with FPN).\n",
    "The segmentation is carried out by determining the coverage of cells to rows and columns and is rule-based.\n",
    "\n",
    "Cell recognition was carried out on the [**PubTabNet**](https://github.com/ibm-aur-nlp/PubTabNet) dataset. PubTabNet contains approx. 500K tables from the field of medical research.\n",
    "\n",
    "We want to try to fine-tune cell recognition on a dataset that comes from a completely different domain, namely financial reports. For this we use [**FinTabNet**](https://arxiv.org/pdf/2005.00589.pdf), a data set that contains around 100K tables from that domain.\n",
    "\n",
    "With this we show what is fundamentally necessary to fine-tune a detector in a pipeline component.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "In order to quickly fine-tune your own data set, you should create your own data set based on the example of the existing data sets. We use the DatasetRegistry to be able to access the dataset directly. Before we start fine tuning, let's take a look at the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from deep_doctection.utils import get_weights_dir_path,get_configs_dir_path\n",
    "from deep_doctection.datasets import DatasetRegistry\n",
    "from deep_doctection.mapper import to_page\n",
    "from deep_doctection.dataflow import MapData\n",
    "from deep_doctection.train import train_faster_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fintabnet', 'testlayout', 'publaynet', 'pubtabnet', 'xfund']\n"
     ]
    }
   ],
   "source": [
    "DatasetRegistry.print_dataset_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FinTabNet dataset contains complex tables from the annual reports of S&P 500 companies with detailed table structure annotations to help train and test structure recognition. To generate the cell structure labels, one uses token matching between the PDF and HTML version of each article from public records and filings. Financial tables often have diverse styles when compared to ones in scientific and government documents, with fewer graphical lines and larger gaps within each table and more colour variations. Fintabnet can be used for training cell detection models as well as for semantic table understanding algorithms. For detection it has cell bounding box annotations as well as precisely described table semantics like row - and column numbers and row and col spans. The dataflow builder can also return captions of bounding boxes of rows and columns. Moreover, various filter conditions on the table structure are available: maximum cell numbers, maximal row and column numbers and their minimum equivalents can be used as filter condition. Header information of cells are not available. As work around you can artificially add header sub-category to every first row cell. All later row cells will receive a no header  sub-category. Note, that this assumption will generate noise.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fintabnet = DatasetRegistry.get_dataset(\"fintabnet\")\n",
    "fintabnet.dataset_info.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the in depths tutorial for more details about the construction of datasets and the architecture of Deep-Doctection. Nevertheless, we will briefly go into the individual steps to display a sample from Fintabnet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1129 12:17.21 @fintabnet.py:158]\u001b[0m \u001b[32mINF\u001b[0m Logic will currently display only ONE table per page, even if there are more !!\n"
     ]
    }
   ],
   "source": [
    "df = fintabnet.dataflow.build(split=\"train\",load_image=True,use_multi_proc=False)\n",
    "df = MapData(df,to_page)\n",
    "df.reset_state()\n",
    "df_iter = iter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Datasets have dataflow components. Dataflows allow efficient loading and mapping of data and thus represent the bloodstream of deep doctection. The build method creates a dataflow of the dataset. By selecting certain parameters, for example, a split can be selected or it can be determined whether the underlying image should be loaded.\n",
    "\n",
    "2.) In the second line, the core data model is mapped to an easily consumable page object. Parsed results can be queried and visualized in the page object.\n",
    "\n",
    "3.) The reset_state () method of the dataflow must be called before iterating the dataflow and belongs to the Dataflow API.\n",
    "\n",
    "4.) We want to use the next method to look at samples, so we create an iterator.\n",
    "\n",
    "After we have created a page object, we enter the annotations in the image with viz () and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=next(df_iter)\n",
    "image = table.viz()\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the diagram, no embedded tables are required, but cut-out tables. A corresponding data flow can be generated by passing the build_mode parameter with \"table\". The coordinates of the cells are converted to the coordinate system of the cropped image.\n",
    "\n",
    "For the training, we use a training script, the content of which corresponds to the Faster-RCNN from Tensorpack, but is adapted to the Deep-Doctection API. Let's collect all necessary inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_yaml=os.path.join(get_configs_dir_path(),\"dd/conf_frcnn_cell.yaml\")\n",
    "dataset_train = fintabnet\n",
    "path_weights = os.path.join(get_weights_dir_path(),\"cell/model-2840000.data-00000-of-00001\")\n",
    "config_overwrite=[\"TRAIN.STEPS_PER_EPOCH=500\",\"TRAIN.EVAL_PERIOD=20\",\"TRAIN.BASE_LR=1e-3\",\"TRAIN.LR_SCHEDULE=[50000]\",\"PREPROC.TRAIN_SHORT_EDGE_SIZE=[600,1200]\"]\n",
    "build_train_config=[\"build_mode=table\",\"load_image=True\",\"max_datapoints=15000\",\"use_multi_proc=False\"]\n",
    "dataset_val = fintabnet\n",
    "build_val_config = [\"build_mode=table\",\"load_image=True\",\"max_datapoints=1000\",\"use_multi_proc=False\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose the LR and the preproc resizing as in  https://github.com/tensorpack/tensorpack/blob/master/examples/FasterRCNN/BALLOON.md. \n",
    "\n",
    "Note that, as we crop the tables off the document image we have to store the cropped image in memory. Hence choose carefully max_datapoints according to your RAM available. You will see a notification in the training protocol\n",
    "\n",
    "[1129 10:51.23 @logger.py:193] WRN Datapoint have images as np arrays stored and they will be loaded into memory. To avoid OOM set 'load_image'=False in dataflow build config. This will load images when needed and reduce memory costs!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_faster_rcnn(path_config_yaml=path_config_yaml,\n",
    "                  dataset_train=fintabnet,\n",
    "                  path_weights=path_weights,\n",
    "                  config_overwrite=config_overwrite,\n",
    "                  log_dir=\"/home/janis/Documents/test_train\",\n",
    "                  build_train_config=build_train_config,\n",
    "                  dataset_val=dataset_val,\n",
    "                  build_val_config=build_val_config,\n",
    "                  metric_name=\"coco\",\n",
    "                  pipeline_component_name=\"ImageLayoutService\"\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-doc",
   "language": "python",
   "name": "deep-doc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
