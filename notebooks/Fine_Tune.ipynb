{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We show how a model can be fine-tuned for a specific task and how the performance can be compared with the pre-trained model.\n",
    "\n",
    "For this purpose, we want to try to improve the table extraction in the **deep**doctection analyzer as an example. To better understand what we are trying to address, we need to say a little more about processing table extraction.\n",
    "\n",
    "\n",
    "![title](./pics/dd_table.png)\n",
    "\n",
    "\n",
    "Table extraction is carried out in different stages:\n",
    "\n",
    "- Table detection\n",
    "- Cell detection\n",
    "- Row and column detection\n",
    "- Segmentation / cell labeling\n",
    "\n",
    "Tables, cells and rows / columns are recognized with object detectors (Cascade-RCNN with FPN).\n",
    "The segmentation is carried out by determining the coverage of cells to rows and columns and is rule-based.\n",
    "\n",
    "Cell recognition was carried out on the [**PubTabNet**](https://github.com/ibm-aur-nlp/PubTabNet) dataset. PubTabNet contains approx. 500K tables from the field of medical research.\n",
    "\n",
    "The current model was configured in such a way that not only table cells are recognized, but header cells are also differentiated from body cells.\n",
    "\n",
    "We want to train the model further on the same dataset. However, we want to replace the header of the model and only recognize cells without any further distinction. Furthermore, want to keep the number of pixels in the image larger in the hope that relatively small cells can be better recognized.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "In order to fine-tune on your own data set, you should create your one dataset instance based on the example of the existing datasets. We use a DatasetRegistry to be able to access the dataset directly. Before we start fine tuning, let's take a look at the dataset. It will show you the advantage of the concept within this framework and how it easily integrates into the training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from deep_doctection.utils import get_weights_dir_path,get_configs_dir_path\n",
    "from deep_doctection.datasets import DatasetRegistry\n",
    "from deep_doctection.mapper import to_page\n",
    "from deep_doctection.dataflow import MapData\n",
    "from deep_doctection.train import train_faster_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fintabnet', 'funsd', 'testlayout', 'publaynet', 'pubtabnet', 'xfund']\n"
     ]
    }
   ],
   "source": [
    "DatasetRegistry.print_dataset_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PubTabNet is a large dataset for image-based table recognition, containing 568k+ images of tabular data annotated with the corresponding HTML representation of the tables. The table images are extracted from the scientific publications included in the PubMed Central Open Access Subset (commercial use collection). Table regions are identified by matching the PDF format and the XML format of the articles in the PubMed Central Open Access Subset. More details are available in our paper 'Image-based table recognition: data, model, and evaluation'. Pubtabnet can be used for training cell detection models as well as for semantic table understanding algorithms. For detection it has cell bounding box annotations as well as precisely described table semantics like row - and column numbers and row and col spans. Moreover, every cell can be classified as header or non-header cell. The dataflow builder can also return captions of bounding boxes of rows and columns. Moreover, various filter conditions on the table structure are available: maximum cell numbers, maximal row and column numbers and their minimum equivalents can be used as filter condition\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubtabnet = DatasetRegistry.get_dataset(\"pubtabnet\")\n",
    "pubtabnet.dataset_info.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the in depths tutorial for more details about the construction of datasets and the architecture of **deep**doctection. Nevertheless, we will briefly go into the individual steps to display a sample from Pubtabnet. The dataset has a method dataflow.build that returns a generator where samples can be streamed from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Dump annotation first or pass external_id to create an annotation id",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-3cfaa8779b97>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Public/deepdoctection/deep_doctection/datapoint/image.py\u001b[0m in \u001b[0;36mas_dict\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m         \u001b[0mimg_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_factory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Public/deepdoctection/deep_doctection/datapoint/convert.py\u001b[0m in \u001b[0;36mas_dict\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_factory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"remove_keys\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Public/deepdoctection/deep_doctection/datapoint/convert.py\u001b[0m in \u001b[0;36mas_dict\u001b[0;34m(obj, dict_factory)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfields\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict_factory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"remove_keys\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mattribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove_keys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Public/deepdoctection/deep_doctection/datapoint/annotation.py\u001b[0m in \u001b[0;36mannotation_id\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotation_id\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_annotation_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dump annotation first or pass external_id to create an annotation id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mannotation_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Dump annotation first or pass external_id to create an annotation id"
     ]
    }
   ],
   "source": [
    "df = pubtabnet.dataflow.build(split=\"train\")\n",
    "df.reset_state()\n",
    "df_iter = iter(df)\n",
    "next(df_iter).as_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Datasets have dataflow components. Dataflows allow efficient loading and mapping of data and thus represent the bloodstream of deep doctection. The build method creates a dataflow of the dataset. By selecting certain parameters, for example, a split can be selected or it can be determined whether the underlying image should be loaded.\n",
    "\n",
    "2.) In the second line, the core data model is mapped to an easily consumable page object. Parsed results can be queried and visualized in the page object.\n",
    "\n",
    "3.) The reset_state () method of the dataflow must be called before iterating the dataflow and belongs to the Dataflow API.\n",
    "\n",
    "4.) We want to use the next method to look at samples, so we create an iterator.\n",
    "\n",
    "After we have created a page object, we enter the annotations in the image with viz () and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=next(df_iter)\n",
    "image = table.viz()\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://github.com/deepdoctection/deepdoctection/raw/master/docs/tutorials/pics/output_7_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample shows that tables are not cropped from the image. Our method however requires that the cell predictor requires tables only without surronding. The build method for this particular dataset, however has a optional parameter `build_mode` such that the dataflow returns tables only, once you pass the value 'table'. The coordinates of the cells are also converted to the coordinate system of the cropped image.\n",
    "\n",
    "\n",
    "## Training Tensorpack Predictor\n",
    "\n",
    "For the training, we use a training script which stems from the training of the Faster-RCNN model from Tensorpack. Let's collect all necessary inputs:\n",
    "\n",
    "- We take the model config for the cell detector. It is important to note that the hyperparameter for this detector differs slightly from the standard Faster-RCNN config for taking into account that cells are generally     smaller and have in general a length/height ratio >=1.\n",
    "- We take the pre-trained cell weights.\n",
    "- We choose fintabnet for training and evaluation for which the datasets is pre-splitted.\n",
    "- Config overwrites specifies the train setting. We define 1 epoch to have 500 pass throughs. (Note, that the term of one epoch is different than in most of the literature.) We set the learning rate to be .001 which is quite common for fine tuning this model. We set the schedule to have 50K iterations and set a random resizing of the train dataset between 600 and 1.2K pixels.\n",
    "- The other settings affect the dataset dataflow build config. As already mentioned we need to crop tables from the images. As we crop them at the beginning, we have to store these images in memory. This is not ideal but peculiar for this dataset and requires either a large memory or a small training dataset. We choose 15k samples. As we only use parts of the images this might ok, but you can set this setting lower if you RAM is not that large.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_yaml=os.path.join(get_configs_dir_path(),\"tp/cell/conf_frcnn_cell.yaml\")\n",
    "path_weights = os.path.join(get_weights_dir_path(),\"cell/model-2840000.data-00000-of-00001\")\n",
    "\n",
    "dataset_train = fintabnet\n",
    "\n",
    "config_overwrite=[\"TRAIN.STEPS_PER_EPOCH=500\",\"TRAIN.EVAL_PERIOD=20\",\"TRAIN.BASE_LR=1e-3\",\"TRAIN.LR_SCHEDULE=[50000]\",\"PREPROC.TRAIN_SHORT_EDGE_SIZE=[600,1200]\"]\n",
    "build_train_config=[\"build_mode=table\",\"load_image=True\",\"max_datapoints=10000\",\"use_multi_proc=False\"]\n",
    "dataset_val = fintabnet\n",
    "build_val_config = [\"build_mode=table\",\"load_image=True\",\"max_datapoints=1000\",\"use_multi_proc=False\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set a directory to save the checkpoints and log the training progress. For evaluation we use the traditional coco metric which returns mean average precision and mean average recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_faster_rcnn(path_config_yaml=path_config_yaml,\n",
    "                  dataset_train=fintabnet,\n",
    "                  path_weights=path_weights,\n",
    "                  config_overwrite=config_overwrite,\n",
    "                  log_dir=\"/path/to/dir/test_train\",\n",
    "                  build_train_config=build_train_config,\n",
    "                  dataset_val=dataset_val,\n",
    "                  build_val_config=build_val_config,\n",
    "                  metric_name=\"coco\",\n",
    "                  pipeline_component_name=\"ImageLayoutService\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-doc",
   "language": "python",
   "name": "deep-doc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
