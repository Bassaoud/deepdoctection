{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We show how a model can be fine-tuned for a specific task and how the performance can be compared with the pre-trained model.\n",
    "\n",
    "For this purpose, we want to try to improve the table extraction in the **deep**doctection analyzer as an example. To better understand what we are trying to address, we need to say a little more about processing table extraction.\n",
    "\n",
    "\n",
    "![title](./pics/dd_table.png)\n",
    "\n",
    "\n",
    "Table extraction is carried out in different stages:\n",
    "\n",
    "- Table detection\n",
    "- Cell detection\n",
    "- Row and column detection\n",
    "- Segmentation / cell labeling\n",
    "\n",
    "Tables, cells and rows / columns are recognized with object detectors (Cascade-RCNN with FPN).\n",
    "The segmentation is carried out by determining the coverage of cells to rows and columns and is rule-based.\n",
    "\n",
    "Cell recognition was carried out on the [**PubTabNet**](https://github.com/ibm-aur-nlp/PubTabNet) dataset. PubTabNet contains approx. 500K tables from the field of medical research.\n",
    "\n",
    "We want to fine-tune cell recognition on a dataset that comes from a completely different domain, namely financial reports. For this we use [**FinTabNet**](https://arxiv.org/pdf/2005.00589.pdf), a data set that contains around 100K tables from that domain.\n",
    "\n",
    "\n",
    "## Dataset\n",
    "\n",
    "In order to fine-tune on your own data set, you should create your one dataset instance based on the example of the existing datasets. We use a DatasetRegistry to be able to access the dataset directly. Before we start fine tuning, let's take a look at the dataset. It will show you the advantage of the concept within this framework and how it easily integrates into the training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from deep_doctection.utils import get_weights_dir_path,get_configs_dir_path\n",
    "from deep_doctection.datasets import DatasetRegistry\n",
    "from deep_doctection.mapper import to_page\n",
    "from deep_doctection.dataflow import MapData\n",
    "from deep_doctection.train import train_faster_rcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fintabnet', 'funsd', 'testlayout', 'publaynet', 'pubtabnet', 'xfund']\n"
     ]
    }
   ],
   "source": [
    "DatasetRegistry.print_dataset_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FinTabNet dataset contains complex tables from the annual reports of S&P 500 companies with detailed table structure annotations to help train and test structure recognition. To generate the cell structure labels, one uses token matching between the PDF and HTML version of each article from public records and filings. Financial tables often have diverse styles when compared to ones in scientific and government documents, with fewer graphical lines and larger gaps within each table and more colour variations. Fintabnet can be used for training cell detection models as well as for semantic table understanding algorithms. For detection it has cell bounding box annotations as well as precisely described table semantics like row - and column numbers and row and col spans. The dataflow builder can also return captions of bounding boxes of rows and columns. Moreover, various filter conditions on the table structure are available: maximum cell numbers, maximal row and column numbers and their minimum equivalents can be used as filter condition. Header information of cells are not available. As work around you can artificially add header sub-category to every first row cell. All later row cells will receive a no header  sub-category. Note, that this assumption will generate noise.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fintabnet = DatasetRegistry.get_dataset(\"fintabnet\")\n",
    "fintabnet.dataset_info.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the in depths tutorial for more details about the construction of datasets and the architecture of **deep**doctection. Nevertheless, we will briefly go into the individual steps to display a sample from Fintabnet. The dataset has a method dataflow.build that returns a generator where samples can be streamed from. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1216 18:55.28 @fintabnet.py:160]\u001b[0m \u001b[32mINF\u001b[0m Logic will currently display only ONE table per page, even if there are more !!\n"
     ]
    }
   ],
   "source": [
    "df = fintabnet.dataflow.build(split=\"train\",load_image=True,use_multi_proc=False)\n",
    "df = MapData(df,to_page)\n",
    "df.reset_state()\n",
    "df_iter = iter(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.) Datasets have dataflow components. Dataflows allow efficient loading and mapping of data and thus represent the bloodstream of deep doctection. The build method creates a dataflow of the dataset. By selecting certain parameters, for example, a split can be selected or it can be determined whether the underlying image should be loaded.\n",
    "\n",
    "2.) In the second line, the core data model is mapped to an easily consumable page object. Parsed results can be queried and visualized in the page object.\n",
    "\n",
    "3.) The reset_state () method of the dataflow must be called before iterating the dataflow and belongs to the Dataflow API.\n",
    "\n",
    "4.) We want to use the next method to look at samples, so we create an iterator.\n",
    "\n",
    "After we have created a page object, we enter the annotations in the image with viz () and visualize them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table=next(df_iter)\n",
    "image = table.viz()\n",
    "plt.figure(figsize = (20,10))\n",
    "plt.axis('off')\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](https://github.com/deepdoctection/deepdoctection/raw/master/docs/tutorials/pics/output_7_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sample shows that tables are not cropped from the image. Our method however requires that the cell predictor requires tables only without surronding. The build method for this particular dataset, however has a optional parameter `build_mode` such that the dataflow returns tables only, once you pass the value 'table'. The coordinates of the cells are also converted to the coordinate system of the cropped image.\n",
    "\n",
    "\n",
    "## Training Tensorpack Predictor\n",
    "\n",
    "For the training, we use a training script which stems from the training of the Faster-RCNN model from Tensorpack. Let's collect all necessary inputs:\n",
    "\n",
    "- We take the model config for the cell detector. It is important to note that the hyperparameter for this detector differs slightly from the standard Faster-RCNN config for taking into account that cells are generally     smaller and have in general a length/height ratio >=1.\n",
    "- We take the pre-trained cell weights.\n",
    "- We choose fintabnet for training and evaluation for which the datasets is pre-splitted.\n",
    "- Config overwrites specifies the train setting. We define 1 epoch to have 500 pass throughs. (Note, that the term of one epoch is different than in most of the literature.) We set the learning rate to be .001 which is quite common for fine tuning this model. We set the schedule to have 50K iterations and set a random resizing of the train dataset between 600 and 1.2K pixels.\n",
    "- The other settings affect the dataset dataflow build config. As already mentioned we need to crop tables from the images. As we crop them at the beginning, we have to store these images in memory. This is not ideal but peculiar for this dataset and requires either a large memory or a small training dataset. We choose 15k samples. As we only use parts of the images this might ok, but you can set this setting lower if you RAM is not that large.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_yaml=os.path.join(get_configs_dir_path(),\"tp/cell/conf_frcnn_cell.yaml\")\n",
    "path_weights = os.path.join(get_weights_dir_path(),\"cell/model-2840000.data-00000-of-00001\")\n",
    "\n",
    "dataset_train = fintabnet\n",
    "\n",
    "config_overwrite=[\"TRAIN.STEPS_PER_EPOCH=500\",\"TRAIN.EVAL_PERIOD=20\",\"TRAIN.BASE_LR=1e-3\",\"TRAIN.LR_SCHEDULE=[50000]\",\"PREPROC.TRAIN_SHORT_EDGE_SIZE=[600,1200]\"]\n",
    "build_train_config=[\"build_mode=table\",\"load_image=True\",\"max_datapoints=10000\",\"use_multi_proc=False\"]\n",
    "dataset_val = fintabnet\n",
    "build_val_config = [\"build_mode=table\",\"load_image=True\",\"max_datapoints=1000\",\"use_multi_proc=False\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we set a directory to save the checkpoints and log the training progress. For evaluation we use the traditional coco metric which returns mean average precision and mean average recall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1216 18:55.36 @logger.py:93]\u001b[0m \u001b[32mINF\u001b[0m Argv: ['/usr/lib/python3/dist-packages/ipykernel_launcher.py', '-f', '/home/janis/.local/share/jupyter/runtime/kernel-60921085-da62-4658-8f59-98ccb0e43f95.json'] \n",
      "\u001b[32m[1216 18:55.37 @config.py:320]\u001b[0m \u001b[32mINF\u001b[0m Environment Information:\n",
      " -----------------------  ------------------------------------------------------------------------------------------------\n",
      "sys.platform             linux\n",
      "Python                   3.8.10 (default, Sep 28 2021, 16:10:42) [GCC 9.3.0]\n",
      "Tensorpack               v0.11-0-gdb541e8e @/home/janis/Public/deepdoctection/venv/lib/python3.8/site-packages/tensorpack\n",
      "Numpy                    1.21.4\n",
      "TensorFlow               2.4.1/unknown @/usr/lib/python3/dist-packages/tensorflow\n",
      "TF Compiler Version      9.3.0\n",
      "TF CUDA support          True\n",
      "TF MKL support           False\n",
      "TF XLA support           False\n",
      "Nvidia Driver            /usr/lib/x86_64-linux-gnu/libnvidia-ml.so.460.73.01\n",
      "CUDA libs                /usr/lib/x86_64-linux-gnu/libcudart.so.11.1.74\n",
      "CUDNN libs\n",
      "TF compute capabilities  52,60,61,70,75,80,86\n",
      "TF built with CUDA       11.1\n",
      "TF built with CUDNN      8\n",
      "NCCL libs                /usr/lib/x86_64-linux-gnu/libnccl.so.2.8.4\n",
      "CUDA_VISIBLE_DEVICES     Unspecified\n",
      "GPU 0                    GeForce RTX 3090\n",
      "Free RAM                 48.37/62.52 GB\n",
      "CPU Count                24\n",
      "Horovod                  0.22.1 @/usr/local/lib/python3.8/dist-packages/horovod\n",
      "cv2                      4.5.3\n",
      "msgpack                  1.0.2\n",
      "python-prctl             True\n",
      "-----------------------  ------------------------------------------------------------------------------------------------\n",
      "\u001b[32m[1216 18:55.37 @config.py:353]\u001b[0m \u001b[32mINF\u001b[0m Warm Up Schedule (steps, value): [(0, 1e-05), (1000, 0.001)]\n",
      "\u001b[32m[1216 18:55.37 @config.py:354]\u001b[0m \u001b[32mINF\u001b[0m LR Schedule (epochs, value): [(2, 0.001)]\n",
      "\u001b[32m[1216 18:55.37 @config.py:357]\u001b[0m \u001b[32mINF\u001b[0m Config: ------------------------------------------\n",
      " {'BACKBONE': {'BOTTLENECK': 'resnext_32xd4',\n",
      "              'FREEZE_AFFINE': False,\n",
      "              'FREEZE_AT': 2,\n",
      "              'NORM': 'GN',\n",
      "              'RESNET_NUM_BLOCKS': [3, 4, 6, 3],\n",
      "              'TF_PAD_MODE': False},\n",
      " 'CASCADE': {'BBOX_REG_WEIGHTS': [[10.0, 10.0, 5.0, 5.0], [20.0, 20.0, 10.0, 10.0],\n",
      "                                  [30.0, 30.0, 15.0, 15.0]],\n",
      "             'IOUS': [0.5, 0.6, 0.7]},\n",
      " 'DATA': {'CLASS_DICT': {0: 'BG', '1': 'TABLE', '2': 'CELL', '3': 'ITEM'},\n",
      "          'CLASS_NAMES': ['TABLE', 'CELL', 'ITEM', 'BG'],\n",
      "          'NUM_CATEGORY': 3,\n",
      "          'TRAIN_NUM_WORKERS': 12},\n",
      " 'FPN': {'ANCHOR_STRIDES': (4, 8, 16, 32, 64),\n",
      "         'CASCADE': True,\n",
      "         'FRCNN_CONV_HEAD_DIM': 256,\n",
      "         'FRCNN_FC_HEAD_DIM': 1024,\n",
      "         'FRCNN_HEAD_FUNC': 'fastrcnn_2fc_head',\n",
      "         'MRCNN_HEAD_FUNC': 'maskrcnn_up4conv_head',\n",
      "         'NORM': 'GN',\n",
      "         'NUM_CHANNEL': 256,\n",
      "         'PROPOSAL_MODE': 'Level',\n",
      "         'RESOLUTION_REQUIREMENT': 32},\n",
      " 'FRCNN': {'BATCH_PER_IM': 512,\n",
      "           'BBOX_REG_WEIGHTS': [10.0, 10.0, 5.0, 5.0],\n",
      "           'FG_RATIO': 0.25,\n",
      "           'FG_THRESH': 0.5},\n",
      " 'MODE_MASK': False,\n",
      " 'MRCNN': {'ACCURATE_PASTE': True, 'HEAD_DIM': 256},\n",
      " 'NUM_GPUS': 1,\n",
      " 'OUTPUT': {'FRCNN_NMS_THRESH': 0.01,\n",
      "            'NMS_THRESH_CLASS_AGNOSTIC': 0.001,\n",
      "            'RESULTS_PER_IM': 800,\n",
      "            'RESULT_SCORE_THRESH': 0.4},\n",
      " 'PREPROC': {'MAX_SIZE': 1408.0,\n",
      "             'PIXEL_MEAN': [238.32, 238.22, 238.21],\n",
      "             'PIXEL_STD': [8.75, 8.569, 9.14],\n",
      "             'SHORT_EDGE_SIZE': 800,\n",
      "             'TRAIN_SHORT_EDGE_SIZE': [600, 1200]},\n",
      " 'RPN': {'ANCHOR_RATIOS': (0.25, 0.5, 1.0),\n",
      "         'ANCHOR_SIZES': (8, 16, 32, 64, 128),\n",
      "         'ANCHOR_STRIDE': 8,\n",
      "         'BATCH_PER_IM': 512,\n",
      "         'CROWD_OVERLAP_THRESH': 9.99,\n",
      "         'FG_RATIO': 0.5,\n",
      "         'HEAD_DIM': 1024,\n",
      "         'MIN_SIZE': 0,\n",
      "         'NEGATIVE_ANCHOR_THRESH': 0.3,\n",
      "         'NUM_ANCHOR': 15,\n",
      "         'PER_LEVEL_NMS_TOPK': 1000,\n",
      "         'POSITIVE_ANCHOR_THRESH': 0.7,\n",
      "         'POST_NMS_TOPK': 1000,\n",
      "         'PRE_NMS_TOPK': 6000,\n",
      "         'PROPOSAL_NMS_THRESH': 0.7,\n",
      "         'TRAIN_PER_LEVEL_NMS_TOPK': 2000,\n",
      "         'TRAIN_POST_NMS_TOPK': 2000,\n",
      "         'TRAIN_PRE_NMS_TOPK': 12000},\n",
      " 'TAG': 'casc_rcnn_X_32xd4_50_FPN_GN_2FC',\n",
      " 'TRAIN': {'BASE_LR': 0.001,\n",
      "           'CHECKPOINT_PERIOD': 20,\n",
      "           'EVAL_PERIOD': 20,\n",
      "           'LOG_DIR': '/home/janis/Documents/test_train',\n",
      "           'LR_SCHEDULE': [50000],\n",
      "           'NUM_GPUS': 1,\n",
      "           'STARTING_EPOCH': 1,\n",
      "           'STEPS_PER_EPOCH': 500,\n",
      "           'WARMUP': 1000,\n",
      "           'WARMUP_INIT_LR': 1e-05,\n",
      "           'WEIGHT_DECAY': 0.0001},\n",
      " 'TRAINER': 'replicated'}\n",
      "\u001b[32m[1216 18:55.37 @tp_frcnn_train.py:138]\u001b[0m \u001b[32mINF\u001b[0m Loading dataset into memory\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          |0/10000[00:00<?,?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1216 18:55.37 @logger.py:193]\u001b[0m \u001b[5m\u001b[35mWRN\u001b[0m Datapoint have images as np arrays stored and they will be loaded into memory. To avoid OOM set 'load_image'=False in dataflow build config. This will load images when needed and reduce memory costs!!!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|###4      |3402/10000[13:59<37:07, 2.96it/s]"
     ]
    }
   ],
   "source": [
    "train_faster_rcnn(path_config_yaml=path_config_yaml,\n",
    "                  dataset_train=fintabnet,\n",
    "                  path_weights=path_weights,\n",
    "                  config_overwrite=config_overwrite,\n",
    "                  log_dir=\"/path/to/dir/test_train\",\n",
    "                  build_train_config=build_train_config,\n",
    "                  dataset_val=dataset_val,\n",
    "                  build_val_config=build_val_config,\n",
    "                  metric_name=\"coco\",\n",
    "                  pipeline_component_name=\"ImageLayoutService\"\n",
    "                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-doc",
   "language": "python",
   "name": "deep-doc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
