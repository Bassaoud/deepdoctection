{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating and Fine Tuning\n",
    "\n",
    "## Introduction\n",
    "\n",
    "We show how a model can be fine-tuned for a specific task and how the performance can be compared with the pre-trained model.\n",
    "\n",
    "For this purpose, we want to try to improve the table extraction in the **deep**doctection analyzer as an example. To better understand what we are trying to address, we need to say a little more about processing table extraction.\n",
    "\n",
    "\n",
    "![title](./pics/dd_table.png)\n",
    "\n",
    "\n",
    "Table extraction is carried out in different stages:\n",
    "\n",
    "- Table detection\n",
    "- Cell detection\n",
    "- Row and column detection\n",
    "- Segmentation / cell labeling\n",
    "\n",
    "Tables, cells and rows / columns are recognized with object detectors (Cascade-RCNN with FPN).\n",
    "The segmentation is carried out by determining the coverage of cells to rows and columns and is rule-based.\n",
    "\n",
    "Cell recognition was carried out on the [**PubTabNet**](https://github.com/ibm-aur-nlp/PubTabNet) dataset. PubTabNet contains approx. 500K tables from the field of medical research.\n",
    "\n",
    "The current model was configured in such a way that not only table cells are recognized, but header cells are also differentiated from body cells.\n",
    "\n",
    "We want to train the model further on the same dataset. However, we want to replace the header of the model and only recognize cells without any further distinction. Furthermore, want to keep the number of pixels in the image larger in the hope that relatively small cells can be better recognized.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "In order to fine-tune on your own data set, you should create your one dataset instance based on the example of the existing datasets. We use a DatasetRegistry to be able to access the built-in dataset directly. Before we start fine tuning, let's take a look at the dataset. It will show you the advantage of the concept within this framework and how it easily integrates into the training scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from deep_doctection.utils import get_weights_dir_path,get_configs_dir_path\n",
    "from deep_doctection.datasets import DatasetRegistry\n",
    "from deep_doctection.eval import MetricRegistry, Evaluator\n",
    "from deep_doctection.extern.tpdetect import TPFrcnnDetector\n",
    "from deep_doctection.pipe.layout import ImageLayoutService"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fintabnet', 'funsd', 'testlayout', 'publaynet', 'pubtabnet', 'xfund']\n"
     ]
    }
   ],
   "source": [
    "DatasetRegistry.print_dataset_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"PubTabNet is a large dataset for image-based table recognition, containing 568k+ images of tabular data annotated with the corresponding HTML representation of the tables. The table images are extracted from the scientific publications included in the PubMed Central Open Access Subset (commercial use collection). Table regions are identified by matching the PDF format and the XML format of the articles in the PubMed Central Open Access Subset. More details are available in our paper 'Image-based table recognition: data, model, and evaluation'. Pubtabnet can be used for training cell detection models as well as for semantic table understanding algorithms. For detection it has cell bounding box annotations as well as precisely described table semantics like row - and column numbers and row and col spans. Moreover, every cell can be classified as header or non-header cell. The dataflow builder can also return captions of bounding boxes of rows and columns. Moreover, various filter conditions on the table structure are available: maximum cell numbers, maximal row and column numbers and their minimum equivalents can be used as filter condition\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pubtabnet = DatasetRegistry.get_dataset(\"pubtabnet\")\n",
    "pubtabnet.dataset_info.description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We refer to the in depths tutorial for more details about the construction of datasets and the architecture of **deep**doctection. Nevertheless, we will briefly go into the individual steps to display a sample from Pubtabnet. The dataset has a method dataflow.build that returns a generator where samples can be streamed from. \n",
    "\n",
    "Let's display a tiny fraction of annotations that is available for each datapoint. `df_dict[\"annotations\"][0]` displays all informations that are available for one cell, i.e. sub categories, like row and column number, header information and bounding boxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('PMC4840965_004_00.png',\n",
       " '/home/janis/Public/deepdoctection/datasets/pubtabnet/train/PMC4840965_004_00.png',\n",
       " 'c87ee674-4ddc-3efe-a74e-dfe25da5d7b3',\n",
       " {'active': True,\n",
       "  'annotation_id': '84cbfafb-c878-323a-afcf-6159206f2e49',\n",
       "  'category_name': 'CELL',\n",
       "  'category_id': '1',\n",
       "  'score': None,\n",
       "  'sub_categories': {'ROW_NUMBER': {'active': True,\n",
       "    'annotation_id': '37cd395e-a09d-3f73-b7e5-98c0d284c75f',\n",
       "    'category_name': 'ROW_NUMBER',\n",
       "    'category_id': '28',\n",
       "    'score': None,\n",
       "    'sub_categories': {},\n",
       "    'relationships': {}},\n",
       "   'COLUMN_NUMBER': {'active': True,\n",
       "    'annotation_id': '626c0980-5a45-3223-b7c8-39bc3648722c',\n",
       "    'category_name': 'COLUMN_NUMBER',\n",
       "    'category_id': '3',\n",
       "    'score': None,\n",
       "    'sub_categories': {},\n",
       "    'relationships': {}},\n",
       "   'ROW_SPAN': {'active': True,\n",
       "    'annotation_id': '02458dd5-e774-3cf6-a299-5546d9c63880',\n",
       "    'category_name': 'ROW_SPAN',\n",
       "    'category_id': '1',\n",
       "    'score': None,\n",
       "    'sub_categories': {},\n",
       "    'relationships': {}},\n",
       "   'COLUMN_SPAN': {'active': True,\n",
       "    'annotation_id': '87df3823-d8f8-3839-ae67-2690f1ff0379',\n",
       "    'category_name': 'COLUMN_SPAN',\n",
       "    'category_id': '1',\n",
       "    'score': None,\n",
       "    'sub_categories': {},\n",
       "    'relationships': {}},\n",
       "   'HEAD': {'active': True,\n",
       "    'annotation_id': '446896bf-f176-349b-bd46-d41aa3397dbb',\n",
       "    'category_name': 'BODY',\n",
       "    'category_id': '<property object at 0x7f102ab0f770>',\n",
       "    'score': None,\n",
       "    'sub_categories': {},\n",
       "    'relationships': {}}},\n",
       "  'relationships': {},\n",
       "  'bounding_box': {'absolute_coords': True,\n",
       "   'ulx': 336.0,\n",
       "   'uly': 381.0,\n",
       "   'lrx': 376.0,\n",
       "   'lry': 391.0},\n",
       "  'image': None})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pubtabnet.dataflow.build(split=\"train\")\n",
    "df.reset_state()\n",
    "df_iter = iter(df)\n",
    "df_dict = next(df_iter).as_dict()\n",
    "df_dict[\"file_name\"],df_dict[\"location\"],df_dict[\"image_id\"], df_dict[\"annotations\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"CELL\" label is the main category. It is possible to change the representation of an annotation by swapping categories with sub categories. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1221 17:51.36 @info.py:205]\u001b[0m \u001b[32mINF\u001b[0m Will reset all previous updates\n"
     ]
    }
   ],
   "source": [
    "pubtabnet.dataflow.categories.set_cat_to_sub_cat({\"CELL\":\"HEAD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'active': True,\n",
       " 'annotation_id': '84cbfafb-c878-323a-afcf-6159206f2e49',\n",
       " 'category_name': 'BODY',\n",
       " 'category_id': '2',\n",
       " 'score': None,\n",
       " 'sub_categories': {'ROW_NUMBER': {'active': True,\n",
       "   'annotation_id': '37cd395e-a09d-3f73-b7e5-98c0d284c75f',\n",
       "   'category_name': 'ROW_NUMBER',\n",
       "   'category_id': '28',\n",
       "   'score': None,\n",
       "   'sub_categories': {},\n",
       "   'relationships': {}},\n",
       "  'COLUMN_NUMBER': {'active': True,\n",
       "   'annotation_id': '626c0980-5a45-3223-b7c8-39bc3648722c',\n",
       "   'category_name': 'COLUMN_NUMBER',\n",
       "   'category_id': '3',\n",
       "   'score': None,\n",
       "   'sub_categories': {},\n",
       "   'relationships': {}},\n",
       "  'ROW_SPAN': {'active': True,\n",
       "   'annotation_id': '02458dd5-e774-3cf6-a299-5546d9c63880',\n",
       "   'category_name': 'ROW_SPAN',\n",
       "   'category_id': '1',\n",
       "   'score': None,\n",
       "   'sub_categories': {},\n",
       "   'relationships': {}},\n",
       "  'COLUMN_SPAN': {'active': True,\n",
       "   'annotation_id': '87df3823-d8f8-3839-ae67-2690f1ff0379',\n",
       "   'category_name': 'COLUMN_SPAN',\n",
       "   'category_id': '1',\n",
       "   'score': None,\n",
       "   'sub_categories': {},\n",
       "   'relationships': {}},\n",
       "  'HEAD': {'active': True,\n",
       "   'annotation_id': 'e9749db2-464c-3144-84f9-f939a4f15a43',\n",
       "   'category_name': 'BODY',\n",
       "   'category_id': '<property object at 0x7f63b575f770>',\n",
       "   'score': None,\n",
       "   'sub_categories': {},\n",
       "   'relationships': {}}},\n",
       " 'relationships': {},\n",
       " 'bounding_box': {'absolute_coords': True,\n",
       "  'ulx': 336.0,\n",
       "  'uly': 381.0,\n",
       "  'lrx': 376.0,\n",
       "  'lry': 391.0},\n",
       " 'image': None}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pubtabnet.dataflow.build(split=\"train\")\n",
    "df.reset_state()\n",
    "df_iter = iter(df)\n",
    "df_dict = next(df_iter).as_dict()\n",
    "df_dict[\"annotations\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation\n",
    "\n",
    "We want to evaluate the current model and use the evaluator framework for this. An evaluator needs a dataset on which to run the evaluation, as well as a predictor and a metric. The predictor must be wraped into a pipeline component, which is why we use the ImageLayoutService.\n",
    "\n",
    "We take the COCO metric for the problem, but define settings that deviate from the standard. We have to consider the following issues, which differ from ordinary object detection tasks:\n",
    "\n",
    "- The objects to be identified are generally smaller\n",
    "- There are many objects to identify."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "coco_metric = MetricRegistry.get_metric(\"coco\")\n",
    "coco_metric.set_params(max_detections=[50,200,600], area_range=[[0,1000000],[0,200],[200,800],[800,1000000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_config_yaml=os.path.join(get_configs_dir_path(),\"tp/cell/conf_frcnn_cell.yaml\")\n",
    "path_weights = os.path.join(get_weights_dir_path(),\"cell/model-2840000.data-00000-of-00001\")\n",
    "\n",
    "\n",
    "categories = pubtabnet.dataflow.categories.get_categories(filtered=True)\n",
    "cell_detector = TPFrcnnDetector(path_config_yaml,path_weights,categories)\n",
    "\n",
    "layout_service =  ImageLayoutService(cell_detector)\n",
    "evaluator = Evaluator(pubtabnet,layout_service, coco_metric)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start the evaluation with the run method. max_datapoints limits the number of samples in the evaluation to 100 data records. The val split is used by default. If this is not available, it must be given as an argument along with other possible build configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1221 17:52.32 @logger.py:193]\u001b[0m \u001b[32mINF\u001b[0m Loading annotations for 'val' split from Pubtabnet will take some time...\n",
      "\u001b[32m[1221 17:53.14 @logger.py:193]\u001b[0m \u001b[32mINF\u001b[0m dp: 549232 is malformed, err: IndexError,\n",
      "            msg: list assignment index out of range in: <frame at 0x6bdcd40, file '/home/janis/Public/deepdoctection/deep_doctection/mapper/pubstruct.py', line 258, code pub_to_image_uncur> will be filtered\n",
      "\u001b[32m[1221 17:53.15 @eval.py:132]\u001b[0m \u001b[32mINF\u001b[0m Predicting objects...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 99/99 [00:12<00:00,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[1221 17:53.28 @eval.py:137]\u001b[0m \u001b[32mINF\u001b[0m Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating index...\n",
      "index created!\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=7.36s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.12s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = -1.000\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=600 ] = 0.930\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=600 ] = 0.768\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=600 ] = 0.590\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=600 ] = 0.689\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=600 ] = 0.644\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 50 ] = 0.584\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=200 ] = 0.708\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=600 ] = 0.711\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=600 ] = 0.665\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=600 ] = 0.741\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=600 ] = 0.695\n"
     ]
    }
   ],
   "source": [
    "output= evaluator.run(category_names=[\"HEAD\",\"BODY\"],max_datapoints=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Tensorpack Predictor\n",
    "\n",
    "For the training, we use a training script that stems from the training of the Faster-RCNN model from Tensorpack. Let's collect all necessary inputs:\n",
    "\n",
    "- We take the model config for the cell detector. It is important to note that the hyperparameter for this detector differs slightly from the standard Faster-RCNN config for taking into account that cells are generally smaller and have in general a length/height ratio >=1.\n",
    "\n",
    "- We take the pre-trained cell weights.\n",
    "\n",
    "- Since we are completely replacing the model head (we are changing the number of categories) we have to plan a longer training schedule. We use the standard training schedule         1xDetectron, which corresponds to a training schedule for a detection task with a pre-trained backbone. This training schedule takes about 2.5 days on a GPU (RTX 3090) and is already included in the configs and therefore does not need to be passed explicitly. The most important training configurations, such as the learning rate schedule, are also derived from this specification. \n",
    "\n",
    "- In the configs we overwrite some configurations for callbacks and the trainable variables:\n",
    "  We train all the variables of the backbone as we change the image size.\n",
    "  We evaluate and save the model every 20 epochs. (Attention: An epoch is defined differently here than the passage of a dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_doctection.train import train_faster_rcnn\n",
    "\n",
    "\n",
    "path_config_yaml=os.path.join(get_configs_dir_path(),\"tp/cell/conf_frcnn_cell.yaml\")\n",
    "path_weights = os.path.join(get_weights_dir_path(),\"cell/model-2840000.data-00000-of-00001\")\n",
    "\n",
    "\n",
    "config_overwrite=[\"TRAIN.EVAL_PERIOD=20\",\"PREPROC.TRAIN_SHORT_EDGE_SIZE=[400,600]\",\"TRAIN.CHECKPOINT_PERIOD=20\",\"BACKBONE.FREEZE_AT=0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other configs refer to dataset and metric settings we discussed before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubtabnet = DatasetRegistry.get_dataset(\"pubtabnet\")\n",
    "pubtabnet.dataflow.categories.filter_categories(categories=\"CELL\")\n",
    "dataset_train = pubtabnet\n",
    "\n",
    "build_train_config=[\"max_datapoints=500000\"]\n",
    "\n",
    "dataset_val = pubtabnet\n",
    "build_val_config = [\"max_datapoints=4000\"]\n",
    "\n",
    "coco_metric = MetricRegistry.get_metric(\"coco\")\n",
    "coco_metric.set_params(max_detections=[50,200,600], area_range=[[0,1000000],[0,200],[200,800],[800,1000000]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_faster_rcnn(path_config_yaml=path_config_yaml,\n",
    "                  dataset_train=pubtabnet,\n",
    "                  path_weights=path_weights,\n",
    "                  config_overwrite=config_overwrite,\n",
    "                  log_dir=\"/path/to/log_dir\",\n",
    "                  build_train_config=build_train_config,\n",
    "                  dataset_val=dataset_val,\n",
    "                  build_val_config=build_val_config,\n",
    "                  metric=coco_metric,\n",
    "                  pipeline_component_name=\"ImageLayoutService\"\n",
    "                  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-doc",
   "language": "python",
   "name": "deep-doc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
