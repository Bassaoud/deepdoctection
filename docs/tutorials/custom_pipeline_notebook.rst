Building a custom pipeline
==========================

In this tutorial we will discuss how to create a pipeline with special
components for text extraction.

This extensive tutorial already discusses many of the core components of
this package.

We formulate the requirement as follows:

**Suppose we want to perform text extraction from complex structured
documents. The documents essentially consist of text blocks and titles.
There are no tables. We want to use the OCR payment service from AWS
Textract. We also want to have a reading order for text blocks, as the
documents contain multiple columns. The analysis results are to be
returned in a JSON structure that contains all layout informations as
well as the full text and the original image.**

Processing steps
----------------

To continue we need to set a processing order. For the construction of
the pipeline, we want to carry out the following steps.

-  Call Textract OCR service
-  Call layout analysis
-  Assign words to layouts blocks via an intersection based rule
-  Determine reading order at the level of layout blocks and at the word
   level within one layout block.

Pipeline component OCR service
------------------------------

A pipeline component is a building block that carries out certain steps
to accomplish a task.

TextExtractionService is a component that calls a selected OCR service
and transforms the returned results into the internal data model. It is
possible to plug in any OCR Detector into the pipeline component. This
allows a certain flexibility with the composition of pipelines.

Important! Textract is an AWS paid service and you will need an AWS
account to call the client. Alternatively, you can also instantiate a
open sourced OCR service like Tesseract. Just disable
``TextractOcrDetector()`` and uncomment the following two lines!

.. code:: ipython3

    import os
    from deepdoctection.extern import TextractOcrDetector, TesseractOcrDetector
    from deepdoctection.pipe import TextExtractionService, DoctectionPipe
    from deepdoctection.utils.systools import get_package_path, get_configs_dir_path

.. code:: ipython3

    ocr_detector = TextractOcrDetector()
    
    #tess_ocr_config_path = os.path.join(get_configs_dir_path(),"dd/conf_tesseract.yaml")
    #ocr_detector = TesseractOcrDetector(tess_ocr_config_path, config_overwrite=["LANGUAGES=deu"])
    
    textract_service = TextExtractionService(ocr_detector,None)
    pipeline_component_list = [textract_service]

Pipeline
--------

We use the DoctectionPipe, which already contains functions for loading
and outputting the extracts.

.. code:: ipython3

    pipeline = DoctectionPipe(pipeline_component_list)

.. code:: ipython3

    path = os.path.join(get_package_path(),"notebooks/pics/samples/sample_3")

.. figure:: ./pics/sample_3.png
   :alt: title

We build the pipeline by calling the analyze method and want the results
returned as an image. An image is the core object where everything
grapped from detectors and pipeline components is stored.

Note, that the default output ‚Äúpage‚Äù will not return anything, as this
type requires additional layout detections which we will adress later.

.. code:: ipython3

    df = pipeline.analyze(path=path, output="image")
    doc = next(iter(df))

It does not make much sense to dig deeper into the image structure. It
is important to know, that it captures all fine graded information from
the OCR result in an ImageAnnotation object. E.g. each single word is
stored with some uuid, bounding box and value (the recorded text).

.. code:: ipython3

    len(doc.annotations), doc.annotations[0]




.. parsed-literal::

    (551,
     ImageAnnotation(active=True, annotation_id='a913d880-af59-302c-a09c-d6cbba4a4ce6', category_name='WORD', category_id='1', score=0.9716712951660156, sub_categories={'CHARS': ContainerAnnotation(active=True, annotation_id='767ad536-9d33-35c5-b9de-b4a7169beebd', category_name='CHARS', category_id='None', score=0.9716712951660156, sub_categories={}, relationships={}, value='Anleihem√§rkte')}, relationships={}, bounding_box=BoundingBox(absolute_coords=True, ulx=137.22318817675114, uly=155.71465119719505, lrx=474.8347396850586, lry=196.48566928505898, height=40.77101808786392, width=337.61155150830746)))



Adding layout elements
----------------------

The current information does not help much so far. An arrangement of
word coordinates from left to right would not result in a meaningful
reading order, as the layout incorporates several columns. One rather
has to determine additional text blocks that frame individual columns. A
built-in layout detector and the associated ImageLayoutService as a
pipeline component are suitable for this.

At this point it starts to depend on whether the DL framework Tensorflow
or PyTorch will be used. We assume that Tensorflow is installed, hence
we need to import the Tensorflow related Detector TPFrcnnDetector. Use
D2FrcnnDetector for PyTorch.

We use the model config and the weights of the built-in analyzer. If you
haven‚Äôt got through the starter tutorial you can download weights using
the ModelDownloadManager.

::

   from deepdoctection.extern.model import ModelDownloadManager
   ModelDownloadManager.maybe_download_weights_and_configs("layout/model-800000_inf_only.data-00000-of-00001")

Download ``"layout/d2_model-800000-layout.pkl"`` instead, in case you
use PyTorch.

.. code:: ipython3

    from deepdoctection.extern import TPFrcnnDetector, ModelCatalog    
    from deepdoctection.pipe import ImageLayoutService
    from deepdoctection.utils.systools import get_weights_dir_path, get_configs_dir_path

When the model is downloaded from the hub, both the weights and the
config file are loaded into the cache. The paths to both files are
required in order to instantiate the detector. You can use the
ModelCatalog to build the path. Moreover, the ModelCatalog provides a
brief model card of all registered models.

It is also necessary to pass a dict with the category-id/category names
pairs. This mapping is standard and results from the dataset Publaynet
on which this model was trained.

.. code:: ipython3

    profile = ModelCatalog.get_profile("layout/model-800000_inf_only.data-00000-of-00001")
    profile.as_dict()




.. parsed-literal::

    {'name': 'layout/model-800000_inf_only.data-00000-of-00001',
     'description': 'Tensorpack layout model for inference purposes trained on Publaynet',
     'size': [274552244, 7907],
     'tp_model': True,
     'config': 'dd/tp/conf_frcnn_layout.yaml',
     'hf_repo_id': 'deepdoctection/tp_casc_rcnn_X_32xd4_50_FPN_GN_2FC_publaynet_inference_only',
     'hf_model_name': 'model-800000_inf_only',
     'hf_config_file': ['conf_frcnn_layout.yaml'],
     'urls': None,
     'categories': {'1': 'TEXT',
      '2': 'TITLE',
      '3': 'LIST',
      '4': 'TABLE',
      '5': 'FIGURE'}}



.. code:: ipython3

    config_yaml_path = ModelCatalog.get_full_path_configs("layout/model-800000_inf_only.data-00000-of-00001")
    weights_path = ModelCatalog.get_full_path_weights("layout/model-800000_inf_only.data-00000-of-00001") 
    categories_layout = profile.categories
    layout_detector = TPFrcnnDetector(config_yaml_path,weights_path,categories_layout)

The ImageLayoutService does need a detector and an additional attribute
that we will not discuss here.

.. code:: ipython3

    layout_service = ImageLayoutService(layout_detector,to_image=True)

Detecting text and layouts are independent tasks, hence the can be
placed in any order within the component.

.. code:: ipython3

    pipeline_component_list.append(layout_service)

Let‚Äôs rebuild a new pipeline and start the process again.

.. code:: ipython3

    pipeline = DoctectionPipe(pipeline_component_list)

.. code:: ipython3

    df = pipeline.analyze(path=path, output="image")
    doc = next(iter(df))
    len(doc.annotations), doc.annotations[0]


.. parsed-literal::

    [32m[0516 15:42:42 @common.py:558][0m [JoinData] Size check failed for the list of dataflow to be joined!

    [32m[0516 15:42.42 @doctectionpipe.py:92][0m [32mINF[0m processing sample_3.png
    [32m[0516 15:42.46 @timer.py:48][0m [32mINF[0m TextExtractionService finished, 3.548 sec.
    [32m[0516 15:42.48 @timer.py:48][0m [32mINF[0m ImageLayoutService finished, 2.4879 sec.

.. parsed-literal::

    (558,
     ImageAnnotation(active=True, annotation_id='a913d880-af59-302c-a09c-d6cbba4a4ce6', category_name='WORD', category_id='1', score=0.9716712951660156, sub_categories={'CHARS': ContainerAnnotation(active=True, annotation_id='767ad536-9d33-35c5-b9de-b4a7169beebd', category_name='CHARS', category_id='None', score=0.9716712951660156, sub_categories={}, relationships={}, value='Anleihem√§rkte')}, relationships={}, bounding_box=BoundingBox(absolute_coords=True, ulx=137.22318817675114, uly=155.71465119719505, lrx=474.8347396850586, lry=196.48566928505898, height=40.77101808786392, width=337.61155150830746)))



Add matching and reading order
------------------------------

Now, that layout and words can be extracted we now have to assign each
detected word to a text box (if this is possible). For that we use the
pre built MatchingService. In our configuration child categories have to
be mapped to parent categories. We use a intersection over are matching
rule with a threshold of 0.9. In other terms, if a word box overlays
with at least 0.9 of its area to a text block it will be assigned to
that box.

.. code:: ipython3

    from deepdoctection.pipe import MatchingService

.. code:: ipython3

    matching_service = MatchingService(parent_categories=["TEXT","TITLE","CELL","LIST","TABLE","FIGURE"],
                            child_categories="WORD",
                            matching_rule="ioa",
                            threshold=0.9)
    
    pipeline_component_list.append(matching_service )

Reading order service has a straight forward setup.

.. code:: ipython3

    from deepdoctection.pipe import TextOrderService

.. code:: ipython3

    reading_order_service = TextOrderService(text_container="WORD",floating_text_block_names=["TEXT","TITLE","LIST"],
                                             text_block_names=["TEXT","TITLE","LIST","TABLE","FIGURE"])

.. code:: ipython3

    pipeline_component_list.append(reading_order_service)

.. code:: ipython3

    pipeline = DoctectionPipe(pipeline_component_list)


We can eventually fire up the custom build analyzer. As we have
everything we need to build the lightweight page object we can change
the output accordingly

.. code:: ipython3

    df = pipeline.analyze(path=path, output="page")
    page = next(iter(df))


.. parsed-literal::

    [32m[0516 15:43:09 @common.py:558][0m [JoinData] Size check failed for the list of dataflow to be joined!

    [32m[0516 15:43.09 @doctectionpipe.py:92][0m [32mINF[0m processing sample_3.png
    [32m[0516 15:43.12 @timer.py:48][0m [32mINF[0m TextExtractionService finished, 3.08 sec.
    [32m[0516 15:43.12 @timer.py:48][0m [32mINF[0m ImageLayoutService finished, 0.1061 sec.
    [32m[0516 15:43.12 @timer.py:48][0m [32mINF[0m MatchingService finished, 0.0093 sec.
    [32m[0516 15:43.12 @timer.py:48][0m [32mINF[0m TextOrderService finished, 0.0294 sec.


We can eventually print the OCRed text in reading order with the
get_text method.

.. code:: ipython3

    print(page.get_text())


.. parsed-literal::

    
    Anleihem√§rkte im Gesch√§ftsjahr bis zum 31.12.2018
    Die internationalen Anleihe- m√§rkte entwickelten sich im Gesch√§ftsjahr 2018 unter- schiedlich und phasenweise sehr volatil. Dabei machte sich bei den Investoren zunehmend Nervosit√§t breit, was in steigen- den Risikopr√§mien zum Aus- druck kam. Grund hierf√ºr waren Turbulenzen auf der weltpoli- tischen B√ºhne, die die politi- schen Risiken erh√∂hten. Dazu z√§hlten unter anderem populis- tische Str√∂mungen nicht nur den USA und Europa, auch den Emerging Markets, wie zuletzt in Brasilien und Mexiko, wo Populisten in die Regie- rungen gew√§hlt wurden. Der eskalierende Handelskonflikt zwischen den USA einerseits sowie Europa und China ande- rerseits tat sein √ºbriges. Zudem ging Italien im Rahmen seiner Haushaltspolitik auf Konfronta- tionskurs zur Europ√§ischen Uni- on (EU). Dar√ºber hinaus verun- sicherte weiterhin der drohende Brexit die Marktteilnehmer, insbesondere dahingehend, ob der m√∂gliche Austritt des Ver- einigten K√∂nigreiches aus der EU geordnet oder - ohne ein √úbereinkommen - ungeordnet vollzogen wird. Im Gegensatz zu den politischen Unsicher- heiten standen die bislang eher zuversichtlichen, konventionel- len Wirtschaftsindikatoren. So expandierte die Weltwirtschaft kr√§ftig, wenngleich sich deren Wachstum im Laufe der zwei- ten Jahresh√§lfte 2018 etwas verlangsamte. Die Geldpolitik war historisch gesehen immer noch sehr locker, trotz der welt- weit sehr hohen Verschuldung und der Zinserh√∂hungen der US-Notenbank.
    Entwicklung der Leitzinsen in den USA und im Euroraum % p.a.
    Zinswende nach bei Anleiherenditen? Im Berichtszeitraum kam es an den Anleihem√§rkten - wenn auch uneinheitlich und unter- schiedlich stark ausgepr√§gt - unter Schwankungen zu stei- genden Renditen auf teilweise immer noch sehr niedrigem Niveau, begleitet von nachge- benden Kursen. Dabei konnten sich die Zinsen vor allem in den USA weiter von ihren histori- schen Tiefs l√∂sen. Gleichzeitig wurde die Zentralbankdivergenz zwischen den USA und dem Euroraum immer deutlicher. An- gesichts des Wirtschaftsbooms in den USA hob die US-Noten- bank Fed im Berichtszeitraum den Leitzins in vier Schritten weiter um einen Prozentpunkt auf einen Korridor von 2,25% - 2,50% p. a. an. Die Europ√§ische Zentralbank (EZB) hingegen hielt an ihrer Nullzinspolitik fest und die Bank of Japan belie√ü ihren Leitzins bei -0,10% p. a. Die Fed begr√ºndete ihre Zinser- h√∂hungen mit der Wachstums- beschleunigung und der Voll- besch√§ftigung am Arbeitsmarkt in den USA. Zinserh√∂hungen erm√∂glichten der US-Notenbank einer √úberhitzung der US-Wirt- schaft vorzubeugen, die durch die prozyklische expansive
    Fiskalpolitik des US-Pr√§sidenten Donald Trump in Form von Steuererleichterungen und einer Erh√∂hung der Staatsausgaben noch befeuert wurde. Vor die- sem Hintergrund verzeichneten die US-Bondm√§rkte einen sp√ºr- baren Renditeanstieg, der mit merklichen Kurserm√§√üigungen einherging. Per saldo stiegen die Renditen zehnj√§hriger US- Staatsanleihen auf Jahressicht von 2,4% p.a. auf 3,1% p. a.
    Diese Entwicklung in den USA hatte auf den Euroraum jedoch nur phasenweise und partiell, insgesamt aber kaum einen zinstreibenden Effekt auf Staats- anleihen aus den europ√§ischen Kernm√§rkten wie beispielsweise Deutschland und Frankreich. gaben zehnj√§hrige deutsche Bundesanleihen im Jahresver- lauf 2018 unter Schwankungen per saldo sogar von 0,42% p.a. auf 0,25% p. a. nach. Vielmehr standen die Anleihem√§rkte der Eurol√§nder - insbeson- dere ab dem zweiten Quartal 2018 - unter dem Einfluss der politischen und wirtschaftlichen Entwicklung in der Eurozone, vor allem in den L√§ndern mit hoher Verschuldung und nied- rigem Wirtschaftswachstum. den Monaten Mai und Juni


How to continue
---------------

In the next step we recommend the tutorial **Datasets_and_Eval**. Here,
the data model of the package is explained in more detail. It also
explains how to evaluate the precision of models using labeled data.
